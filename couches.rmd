---
title: "Coasting on Couches"
output: html_document
# runtime: shiny
author: Akshay R, Kevin *Tan*, Wan Ling *Loh*, Zach *Zhang*, Xin Ying *Goh*
editor_options: 
  chunk_output_type: inline
---
# 0. Introduction
This document lists the exploratory data analysis, model build and analysis for *Coasting on Couches*, the term project for MGT 6203 Spring semester. Whilst the project document and slides/ presentation list a more human-readable version, this document combines some exposition with a lot of code to generate graphs, to put it simply.


## 0.1 Library Setup
The document will be shared as a Shiny app and in the raw RMd format. 

Please run the following chunk to ensure all the necessary libraries are installed/ present should you wish to execute the chunks at your end. (Idea taken from [this blogpost](https://vbaliga.github.io/verify-that-r-packages-are-installed-and-loaded/))

```{r include=FALSE}
packages<- c("dplyr", "stringr", "readr", "stargazer", "knitr", "DataExplorer", "ggplot2", "car","ggExtra", "psych", "PerformanceAnalytics", "corrplot", "mice", "plotly", "geojsonio", "rjson", "geojsonR", "pls", "fastDummies", "Metrics") #add new libraries here

package.check <- lapply(
  packages,
  FUN = function(x)
  {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE)
      library(x, character.only = TRUE)
    }
  }
)
```

Please execute `install.packages("pls")` in the console if it's the first time you're running it. There could be some additional installations, depending on your operating system.
 
# 1. Loading Data

We had downloaded data from [InsideAirbnb.com](http://insideairbnb.com/get-the-data) to the data folder. This will be available on our [GitHub repo](https://github.com/metamutator/crispy-octo-waffle). 

The data is in four parts (each city has all five elements):
1. ``listing``: These are the actual Airbnb listings. The columns are defined [here](https://docs.google.com/spreadsheets/d/1iWCNJcSutYqpULSQHlNyGInUvHg2BoUGoNRIGa6Szc4/edit#gid=982310896). This is set of 74 variables pertaining to a specific listing.
2. ``review``: List of reviews per row in the ``listing`` table.
3. ``calendar``: Price of a particular listing on a particular date, along with min-max nights for hire
4. ``neighbourhoods``: List of neighbourhoods screened in the city
5. ``map``: GeoJson shapefile showing district boundaries.

## 1.1 Raw

We will read all the data into dataset variables. 
```{r echo=TRUE, message=TRUE, warning=TRUE}
#Singapore
listing.sin <- read.csv("./data/SIN_listings.csv")
reviews.sin <- read.csv("./data/SIN_reviews.csv")
calendar.sin <- read.csv("./data/SIN_calendar.csv")
neighbourhoods.sin <- read.csv("./data/SIN_neighbourhoods.csv")
map.sin <- geojson_read("./data/SIN_neighbourhoods.geojson")

#Taipei
listing.tpe <- read.csv("./data/TPE_listings.csv")
reviews.tpe <- read.csv("./data/TPE_reviews.csv")
calendar.tpe <- read.csv("./data/TPE_calendar.csv")
neighbourhoods.tpe <- read.csv("./data/TPE_neighbourhoods.csv")
map.tpe <- geojson_read("./data/TPE_neighbourhoods.geojson")

#Tokyo
listing.nrt <- read.csv("./data/NRT_listings.csv")
reviews.nrt <- read.csv("./data/NRT_reviews.csv")
calendar.nrt <- read.csv("./data/NRT_calendar.csv")
neighbourhoods.nrt <- read.csv("./data/NRT_neighbourhoods.csv")
map.nrt <- geojson_read("./data/NRT_neighbourhoods.geojson")

#Hong Kong
listing.hkg <- read.csv("./data/HKG_listings.csv")
reviews.hkg <- read.csv("./data/HKG_reviews.csv")
calendar.hkg <- read.csv("./data/HKG_calendar.csv")
neighbourhoods.hkg <- read.csv("./data/HKG_neighbourhoods.csv")
map.hkg <- geojson_read("./data/HKG_neighbourhoods.geojson")
```

## 1.2 Initial Data Analysis

### 1.2.0 Number of Listings
We first see the number of listings per city.
```{r}
cities  <- c("Singapore", "Tokyo", "Taipei", "Hong Kong")
no_of_listings <- c(nrow(listing.sin), nrow(listing.nrt), nrow(listing.tpe), nrow(listing.hkg))
no_of_listings.fig <- plot_ly(
  x = cities,
  y = no_of_listings,
  type = "bar", 
  text = no_of_listings
)
no_of_listings.fig <- no_of_listings.fig %>% layout(title ="No of Listings Per City", yaxis = list(title="No of Listings"))
no_of_listings.fig
```

Clearly, Tokyo has the largest number of listings followed by Hong Kong, Taipei and then Singapore. 

### 1.2.1 Listings by Neighbourhood - Choropleth Maps

Let us also consider heatmaps of where the listings are in each city by neighbourhood. Admittedly, the InsideAirbnb website has a map for each city (here's one for [Taipei](http://insideairbnb.com/taipei)), but this does not show by district. At a later stage, this can be enhanced to see by variable or time-series.

```{r}
generate_choropleth_by_city <- function (listing, map, city_name)
{
  listings_by_neighbourhood <- listing %>%
                                count(neighbourhood_cleansed) 
  # neighbourhoods_zero <- neighbourhoods %>%
  #                       filter(!neighbourhood %in% listings_by_neighbourhood$neighbourhood) %>%
  #                       mutate(n = 0) %>%
  #                       select(neighbourhood, n)
  # listings_by_neighbourhood <- union(listings_by_neighbourhood, neighbourhoods_zero)
  # print(listings_by_neighbourhood)
  g <- list (
    fitbounds = "locations",
    visible = FALSE
  )
  fig <- plot_ly()
  fig <- fig %>% add_trace(
    type="choropleth",
    geojson=map,
    locations=listings_by_neighbourhood$neighbourhood_cleansed,
    z=listings_by_neighbourhood$n,
    colorscale="jet",
    featureidkey="properties.neighbourhood"
  )
  
  fig <- fig %>% layout(
    geo = g
  )
  fig <- fig %>% colorbar(title = "No of listings")
  fig <- fig %>% layout(
    title = paste0("Listings by Neighbourhood - ", city_name)
  )
  fig
}
generate_choropleth_by_city(listing.sin, map.sin, "Singapore")
generate_choropleth_by_city(listing.nrt, map.nrt, "Tokyo")
generate_choropleth_by_city(listing.hkg, map.hkg, "Hong Kong")
generate_choropleth_by_city(listing.tpe, map.tpe, "Taipei")
```

```{r}
bin_districts <- function(listing, bins)
{
  district_bins <- listing %>%
                    count(neighbourhood_cleansed) %>%
                    arrange(desc(n))%>%
                    mutate(nb_group = ntile(n,n=bins)) %>%
                    arrange(desc(nb_group))
  return(district_bins)
}

bin_districts(listing.sin, 4)
```
### 1.2.2 Listings by Neighbourhood - Bar Charts

We could also see this as barcharts.
```{r}
bar_charts_by_neighbourhood <- function (listing, city_name, neighbourhoods)
{
  listings_by_neighbourhood <- listing %>%
                                count(neighbourhood_cleansed) %>%
                                # rename(neighbourhood = neighbourhood_cleansed)
                                arrange(desc(n))
  # print(listings_by_neighbourhood)
  neighbourhoods_zero <- neighbourhoods %>%
                        filter(!neighbourhood %in% listings_by_neighbourhood$neighbourhood_cleansed) %>%
                        rename(neighbourhood_cleansed = neighbourhood) %>%
                        mutate(n = 0) %>%
                        select(neighbourhood_cleansed, n)
  print(neighbourhoods_zero)
  listings_by_neighbourhood <- union(listings_by_neighbourhood, neighbourhoods_zero)
  # print(listings_by_neighbourhood)
  fig<- plot_ly(y=listings_by_neighbourhood$neighbourhood_cleansed, x=listings_by_neighbourhood$n, type="bar", orientation="h") %>%
        layout(yaxis=list(categoryorder = "total ascending"), title=paste("Listings per neighbourhood in", city_name))
  fig
}

bar_charts_by_neighbourhood(listing.sin, "Singapore", neighbourhoods.sin)
bar_charts_by_neighbourhood(listing.nrt, "Tokyo", neighbourhoods.nrt)
bar_charts_by_neighbourhood(listing.hkg, "Hong Kong", neighbourhoods.hkg)
bar_charts_by_neighbourhood(listing.tpe, "Taipei", neighbourhoods.tpe)
```

The majority of listings in Taipei, Tokyo and Hong Kong are in tourist-heavy districts. While the top district in Tokyo is Shinjuku, that in Hong Kong is  Yau Tsim Mong, Kowloon's core urban area formed by the combination of Yau Ma Tei, Tsim Sha Tsui and Mong Kok. Taipei's top district is Zhongzheng district ("中正區"), consisting of historic sites and cultural performances.

In contrast to the other three cities, the top district in Singapore is the high-end residential condo district, Kallang. Not tourist heavy, but close to the downtown's many attractions. The tourist-heavy Geylang, Downtown Core and Outram districts appear after Kallang.

Taipei and Hong Kong have listings in all districts. But 11 districts in Singapore (mostly military installations or the airport) and 16 districts in Tokyo do not have any listings. 

### 1.2.3 Amenities

There's a column called amenities in the dataset that appears to list all the self-reported amenities in the listing as a single comma-separated list. Let's try to see this further. 

For instance, here's the longest list of amenities among Singapore listings.
```{r echo=TRUE, message=TRUE, warning=TRUE, paged.print=TRUE}
listing_amenities.sin <- listing.sin %>%
                      mutate(amenities = str_replace(amenities, "\\[","")) %>%
                      mutate(amenities = str_replace(amenities, "\\]","")) %>%
                      mutate(amenities = str_replace_all(amenities, "\"","")) %>%
                      mutate(amenities = str_replace_all(amenities, ", " ,",")) %>%
                      mutate(amenities_list = as.list(strsplit(amenities, ","))) %>%
                      mutate(no_of_am = lengths(amenities_list)) %>%
                      mutate(Wifi = as.numeric(grepl('Wifi', amenities, fixed = TRUE))) %>%
                      mutate(Shampoo = as.numeric(grepl('Shampoo', amenities, fixed = TRUE))) %>%
                      mutate(Kitchen = as.numeric(grepl('Kitchen', amenities, fixed = TRUE))) 

# listing_amenities.sin %>% select(amenities, Wifi, Shampoo, Kitchen, Patio)

max_amenities.sin <- listing_amenities.sin %>%
                      select(amenities, no_of_am) %>%
                      group_by() %>%
                     slice(which.max(no_of_am))
amenities_list_string <- as.list(strsplit(as.character(max_amenities.sin["amenities"]), ","))
amenities_list_string

#"Shampoo,Kitchen,Long term stays allowed,Washer,Smart lock,Hair dryer,Dryer,Wifi,Hot water,TV,Air conditioning,Smoke alarm,Fire extinguisher"
```

Apropos nothing, we will use the following amenities as dummy variables for price:
> "Shampoo,Kitchen,Long term stays allowed,Washer,Hair dryer,Wifi,Hot water,TV,Air conditioning"

### 1.2.4 Host Verifications

Similarly, let us also further analyse the column `host_verifications` to see if we can generate dummy variables from there as well.
```{r}
listing_host_verf.sin <- listing.sin %>%
                      mutate(host_verifications = str_replace(host_verifications, "\\[","")) %>%
                      mutate(host_verifications = str_replace(host_verifications, "\\]","")) %>%
                      mutate(host_verifications = str_replace_all(host_verifications, "\"","")) %>%
                      mutate(host_verifications = str_replace_all(host_verifications, ", " ,",")) %>%
                      mutate(host_verifications_list = as.list(strsplit(host_verifications, ","))) %>%
                      mutate(no_of_vf = lengths(host_verifications_list))

max_verf.sin <- listing_host_verf.sin %>%
                      select(host_verifications, no_of_vf) %>%
                      group_by() %>%
                     slice(which.max(no_of_vf))
host_verf_list_string <- as.list(strsplit(as.character(max_verf.sin["host_verifications"]), ","))
host_verf_list_string
```
Let's take this list to generate dummy variables.
> ['email', 'phone', 'facebook', 'reviews', 'manual_offline', 'jumio', 'offline_government_id', 'government_id', 'work_email']

Let's generalise these two bits for all cities and create dummy variables for each one of them.

```{r}

wrangle_amenities_hostvf <- function (listing)
{
  listing <- listing %>%
            mutate(amenities = str_replace(amenities, "\\[","")) %>%
            mutate(amenities = str_replace(amenities, "\\]","")) %>%
            mutate(amenities = str_replace_all(amenities, "\"","")) %>%
            mutate(amenities = str_replace_all(amenities, ", " ,",")) %>%
            mutate(amenities_list = as.list(strsplit(amenities, ","))) %>%
            mutate(no_of_am = lengths(amenities_list)) %>%
            mutate(Amenities_Wifi = as.numeric(grepl('Wifi', amenities, fixed = TRUE))) %>%
            mutate(Amenities_Shampoo = as.numeric(grepl('Shampoo', amenities, fixed = TRUE))) %>%
            mutate(Amenities_Kitchen = as.numeric(grepl('Kitchen', amenities, fixed = TRUE))) %>%
            mutate(Amenities_Long_Term = as.numeric(grepl('Long term stays', amenities, fixed = TRUE))) %>%
            mutate(Amenities_Washer = as.numeric(grepl('Washer', amenities, fixed = TRUE))) %>%
            mutate(Amenities_HairDryer = as.numeric(grepl('Hair dryer', amenities, fixed = TRUE))) %>%
            mutate(Amenities_HotWater = as.numeric(grepl('Hot water', amenities, fixed = TRUE))) %>%
            mutate(Amenities_TV = as.numeric(grepl('TV', amenities, fixed = TRUE))) %>%
            mutate(Amenities_AC = as.numeric(grepl('Air conditioning', amenities, fixed = TRUE))) %>%

            mutate(host_verifications = str_replace(host_verifications, "\\[","")) %>%
            mutate(host_verifications = str_replace(host_verifications, "\\]","")) %>%
            mutate(host_verifications = str_replace_all(host_verifications, "\"","")) %>%
            mutate(host_verifications = str_replace_all(host_verifications, ", " ,",")) %>%
            mutate(host_verifications_list = as.list(strsplit(host_verifications, ","))) %>%
            mutate(hv_email = as.numeric(grepl('email', host_verifications, fixed = TRUE))) %>%
            mutate(hv_phone = as.numeric(grepl('phone', host_verifications, fixed = TRUE))) %>%
            mutate(hv_facebook = as.numeric(grepl('facebook', host_verifications, fixed = TRUE))) %>%
            mutate(hv_reviews = as.numeric(grepl('reviews', host_verifications, fixed = TRUE))) %>%
            mutate(hv_manual_offline = as.numeric(grepl('manual_offline', host_verifications, fixed = TRUE))) %>%
            mutate(hv_manual_jumio = as.numeric(grepl('jumio', host_verifications, fixed = TRUE))) %>%
            mutate(hv_manual_off_gov = as.numeric(grepl('offline_government_id', host_verifications, fixed = TRUE))) %>%
            mutate(hv_manual_gov = as.numeric(grepl('government_id', host_verifications, fixed = TRUE))) %>%
            mutate(hv_manual_work_email = as.numeric(grepl('work_email', host_verifications, fixed = TRUE))) %>%
            mutate(no_of_vf = lengths(host_verifications_list))
}

listing.sin <- wrangle_amenities_hostvf(listing.sin)
listing.nrt <- wrangle_amenities_hostvf(listing.nrt)
listing.tpe <- wrangle_amenities_hostvf(listing.tpe)
listing.hkg <- wrangle_amenities_hostvf(listing.hkg)
```

## 1.3 Data Wrangling

### 1.3.1 Wrangling Listings
This is a function that wrangles AirBnb data into an analysable chunk. Because we will be doing the same for multiple cities, we will do a function out of this. The function is based on top of code shared in the lecture for Module 2. The obvious additions are the id column, neighbourhoods and dummy variables for amenities and host verification.
```{r echo=TRUE, warning=FALSE}

wrangle_airbnb_dataset <- function (raw_listing_full)
{
  listing.raw <- raw_listing_full  %>% 
                select(id, price,number_of_reviews,beds,bathrooms,accommodates,reviews_per_month, property_type, room_type, review_scores_rating, neighbourhood_cleansed, host_response_time, host_response_rate, host_acceptance_rate, host_is_superhost, latitude, longitude, amenities, last_review, no_of_am, Amenities_Wifi, Amenities_Shampoo, Amenities_Kitchen, Amenities_Long_Term, Amenities_Washer, Amenities_HairDryer, Amenities_HotWater, Amenities_TV,Amenities_AC, host_verifications, hv_email,hv_phone, hv_facebook, hv_reviews, hv_manual_offline, hv_manual_jumio,hv_manual_off_gov, hv_manual_gov, hv_manual_work_email, no_of_vf) %>% 
                rename(Reviews = number_of_reviews) %>% 
                rename(Beds = beds) %>% 
                rename(Baths = bathrooms) %>% 
                rename(Capacity = accommodates) %>% 
                rename(Monthly_Reviews = reviews_per_month) %>% 
                rename(Property_Type = property_type) %>% 
                rename(Room_Type = room_type) %>% 
                rename(Price = price) %>% 
                rename(Rating = review_scores_rating) %>%
                # rename(Neighbourhood = neighbourhood_cleansed) %>%
                rename(host_Superhost = host_is_superhost)


  listing.raw <-  listing.raw %>% 
                mutate(Price = str_replace(Price, "[$]", "")) %>% 
                mutate(Price = str_replace(Price, "[,]", "")) %>% 
                mutate(Price = as.numeric(Price)) %>% 
                
                # mutate(hood_factor = as.factor(Neighbourhood)) %>%
                
                mutate(host_response_rate = str_replace(host_response_rate, "[%]", "")) %>%
                mutate(host_response_rate = as.numeric(host_response_rate)/100) %>%
                mutate(host_acceptance_rate = str_replace(host_acceptance_rate, "[%]", "")) %>%
                mutate(host_acceptance_rate = as.numeric(host_acceptance_rate)/100) %>%
                mutate(host_Superhost = ifelse(host_Superhost =="f", 0, 1)) %>%
    
                mutate(host_response_rate = factor(host_response_rate, levels = c("within a few hours", "within a day", "a few days or more"))) %>%
                mutate(host_response_hours = ifelse(host_response_rate == "within a few hours"),1,0) %>%
                mutate(host_response_day = ifelse(host_response_rate == "within a day"),1,0) %>%
                mutate(host_response_few_days = ifelse(host_response_rate == "a few days or more"),1,0) %>%
                
                mutate(last_review = as.Date(last_review)) %>%
                mutate(Days_since_last_review = as.numeric(difftime(as.Date("2021-12-31"), last_review, units="days"))) %>%
    
                mutate(Room_Type = factor(Room_Type, levels = c("Shared room", "Private room", "Entire home/apt"))) %>% 
                mutate(Capacity_Sqr = Capacity * Capacity) %>% 
                mutate(Beds_Sqr = Beds * Beds) %>% 
                mutate(Baths_Sqr = Baths * Baths) %>% 
                mutate(ln_Price = log(1+Price)) %>% 
                mutate(ln_Beds = log(1+Beds)) %>%
                mutate(ln_Baths = log(1+Baths)) %>% 
                mutate(ln_Capacity = log(1+Capacity)) %>% 
                mutate(ln_Rating = log(1+Rating)) %>% 
                mutate(Shared_ind = ifelse(Room_Type == "Shared room",1,0)) %>% 
                mutate(House_ind = ifelse(Room_Type == "Entire home/apt",1,0)) %>% 
                mutate(Private_ind = ifelse(Room_Type == "Private room",1,0)) %>% 
                mutate(Capacity_x_Shared_ind = Shared_ind * Capacity) %>% 
                mutate(H_Cap = House_ind * Capacity) %>% 
                mutate(P_Cap = Private_ind * Capacity) %>% 
                mutate(ln_Capacity_x_Shared_ind = Shared_ind * ln_Capacity) %>% 
                mutate(ln_Capacity_x_House_ind = House_ind * ln_Capacity) %>% 
                mutate(ln_Capacity_x_Private_ind = Private_ind * ln_Capacity) %>%
                
                filter(!is.na(Price))

  return(listing.raw)
}

list.sin <- wrangle_airbnb_dataset(listing.sin)
list.nrt <- wrangle_airbnb_dataset(listing.nrt)
list.tpe <- wrangle_airbnb_dataset(listing.tpe)
list.hkg <- wrangle_airbnb_dataset(listing.hkg)
```

### 1.3.2 Wrangling Reviews.
There's value in understanding how many reviews a property has received in the last 12 months as a measure of how active a property is. The notion is that modelling price for active listings will be more accurate than modelling price for _all_ listings. 

The approach taken in [this paper](https://towardsdatascience.com/airbnb-pricing-recommender-19225d0f5d1) was to look listings active in the past 12 months. However, given restrictions because of pandemic, we felt it would be better to look at the period between 1 Jan 2019 and 31 Dec 2021, to include one year in addition to the two pandemic years, 2020 and 2021. 

We check this by wrangling the review dataset.

```{r}
count_reviews <- function(listings, reviews, from_date, to_date)
{
  reviews_grouped <- reviews %>%
                  mutate(date = as.Date(date)) %>%
                  filter(between(date, as.Date(from_date), as.Date(to_date))) %>%
                  group_by(listing_id) %>%
                  summarise(reviews_since_2019 = n()) %>%
                  mutate(bookings_since_2019 = reviews_since_2019*2) %>%
                  rename(id = listing_id)
  listings <- left_join(listings, reviews_grouped, by="id")
  return(listings)
}
start_date = "2019-1-1"
end_date = "2021-12-31"
list.sin <-count_reviews(list.sin, reviews.sin,start_date, end_date)
list.hkg <- count_reviews(list.hkg, reviews.hkg,start_date, end_date)
list.tpe <- count_reviews(list.tpe, reviews.tpe,start_date, end_date)
list.nrt <- count_reviews(list.nrt, reviews.nrt,start_date, end_date)

list_after_2019.sin <- list.sin %>% filter(!is.na(reviews_since_2019)) 
list_after_2019.tpe <- list.tpe %>% filter(!is.na(reviews_since_2019))
list_after_2019.nrt <- list.nrt %>% filter(!is.na(reviews_since_2019))
list_after_2019.hkg <- list.hkg %>% filter(!is.na(reviews_since_2019))
```

Let's try to look listings after 2019 in map form.
```{r}
generate_choropleth_by_city(list_after_2019.sin, map.sin, "Singapore")
generate_choropleth_by_city(list_after_2019.nrt, map.nrt, "Tokyo")
generate_choropleth_by_city(list_after_2019.hkg, map.hkg, "Hong Kong")
generate_choropleth_by_city(list_after_2019.tpe, map.tpe, "Taipei")

bar_charts_by_neighbourhood(list_after_2019.sin, "Singapore", neighbourhoods.sin)
bar_charts_by_neighbourhood(list_after_2019.nrt, "Tokyo", neighbourhoods.nrt)
bar_charts_by_neighbourhood(list_after_2019.hkg, "Hong Kong", neighbourhoods.hkg)
bar_charts_by_neighbourhood(list_after_2019.tpe, "Taipei", neighbourhoods.tpe)
```

```{r}
add_earnings <- function(listing)
{
  return (listing %>% mutate(earnings_since_2019 = bookings_since_2019 * 3 * Price))
}
list_after_2019.sin <- add_earnings(list_after_2019.sin) 
list_after_2019.tpe <- add_earnings(list_after_2019.tpe) 
list_after_2019.nrt <- add_earnings(list_after_2019.nrt) 
list_after_2019.hkg <- add_earnings(list_after_2019.hkg) 
```

Let's group listings into groups of neighbourhoods: extremely popular, popular, moderate, not so popular, and sparse.
```{r}
district_bins.sin <- bin_districts(list_after_2019.sin, bins=5)
district_bins.sin
district_bins.nrt <- bin_districts(list_after_2019.nrt, bins=5)
district_bins.nrt
district_bins.hkg <- bin_districts(list_after_2019.hkg, bins=5)
district_bins.hkg
district_bins.tpe <- bin_districts(list_after_2019.tpe, bins=5)
district_bins.tpe
```

```{r}
list_after_2019.sin <- left_join(list_after_2019.sin, district_bins.sin %>% select(neighbourhood_cleansed, nb_group), by="neighbourhood_cleansed")
list_after_2019.nrt <- left_join(list_after_2019.nrt, district_bins.nrt %>% select(neighbourhood_cleansed, nb_group), by="neighbourhood_cleansed")
list_after_2019.hkg <- left_join(list_after_2019.hkg, district_bins.hkg %>% select(neighbourhood_cleansed, nb_group), by="neighbourhood_cleansed")
list_after_2019.tpe <- left_join(list_after_2019.tpe, district_bins.tpe %>% select(neighbourhood_cleansed, nb_group), by="neighbourhood_cleansed")
# list_after_2019.sin
```

```{r paged.print=TRUE}

list_after_2019.sin <- dummy_cols(list_after_2019.sin, select_columns = "nb_group", remove_selected_columns = TRUE)
list_after_2019.nrt <- dummy_cols(list_after_2019.nrt, select_columns = "nb_group", remove_selected_columns = TRUE)
list_after_2019.tpe <- dummy_cols(list_after_2019.tpe, select_columns = "nb_group", remove_selected_columns = TRUE)
list_after_2019.hkg <- dummy_cols(list_after_2019.hkg, select_columns = "nb_group", remove_selected_columns = TRUE)

list_after_2019.sin_remove <- dummy_cols(list_after_2019.sin, select_columns = c("Property_Type","Room_Type"), remove_selected_columns = TRUE)
list_after_2019.nrt_remove <- dummy_cols(list_after_2019.nrt, select_columns = c("Property_Type","Room_Type"), remove_selected_columns = TRUE)
list_after_2019.tpe_remove <- dummy_cols(list_after_2019.tpe, select_columns = c("Property_Type","Room_Type"), remove_selected_columns = TRUE)
list_after_2019.hkg_remove <- dummy_cols(list_after_2019.hkg, select_columns = c("Property_Type","Room_Type"), remove_selected_columns = TRUE)


# list_after_2019.sin
# list_after_2019.nrt
# list_after_2019.hkg
# list_after_2019.tpe
```
This reduces the number of listings, and hopefully, quite a few outliers. 
```{r}

cities  <- c("Singapore", "Tokyo", "Taipei", "Hong Kong")
no_of_listings <- c(nrow(listing.sin), nrow(listing.nrt), nrow(listing.tpe), nrow(listing.hkg))
no_of_listings_after_2019 <- c(nrow(list_after_2019.sin), nrow(list_after_2019.nrt), nrow(list_after_2019.tpe), nrow(list_after_2019.hkg))
data <- data.frame(cities, no_of_listings, no_of_listings_after_2019)
no_of_listings.fig <- plot_ly(data, 
  x = cities,
  y = ~no_of_listings,
  type = "bar", 
  text = no_of_listings,
  name = "No of Listings (All Years)"
)
no_of_listings.fig <- no_of_listings.fig %>% add_trace(y = ~no_of_listings_after_2019, text= no_of_listings_after_2019, name = "No of Active Listings")
no_of_listings.fig <- no_of_listings.fig %>% layout(title ="No of Listings Per City", yaxis = list(title="No of Listings"))
no_of_listings.fig
```
This roughly halves the number of listings being considered in Hong Kong and Singapore, but not in Taipei and Tokyo.

# 2. Variable Selection: Exploratory Data Analysis

We now attempt to check on variables for each city.

## 2.1 Singapore
```{r}
data_exploration <- function (listing)
{
  plot_str(listing, type="r")
  introduce(listing)
  plot_intro(listing)
  plot_missing(listing)
  plot_bar(listing)
  pca_df <- na.omit(list.sin[, c("Price", "Room_Type", "Reviews", "Beds", "Capacity", "Monthly_Reviews", "host_Superhost", "Rating")])#,"Days_since_last_review", "host_response_rate", "host_response_hours", "host_acceptance_rate","host_response_day", "host_response_few_days")])
  plot_qq(pca_df)
  plot_prcomp(pca_df, variance_cap = 0.9, nrow = 2L, ncol=2L)
}
data_exploration(list.sin)
```
Taipei
```{r}
data_exploration(list.tpe)
```

Hong Kong
```{r}
data_exploration(list.hkg)
```

Tokyo
```{r}
data_exploration(list.nrt)
```

## 2.2 Boxplots

We will now check out outliers in our data for various parameters, filtering for listings that have seen at least one booking since 1 Jan 2019, starting with Singapore data.
```{r}
generate_price_boxplot <- function (listing.clean, city, comparison_col = "")
{
  # png(file = "./graphs/boxplot.png")
  if (comparison_col == "")
  {
    boxplot(listing.clean$Price, data = listing.clean, ylab="Price", main=paste("Boxplot: Price for", city))
  }
  else
    boxplot(listing.clean$Price ~ listing.clean[[comparison_col]], data = listing.clean, ylab="Price", xlab=comparison_col, main=paste("Boxplot: Price vs", comparison_col, "for", city))
  # dev.off()
}

generate_price_boxplot(list_after_2019.sin, "Singapore") #, sin_listing.clean$)
generate_price_boxplot(list_after_2019.sin, "Singapore", "Room_Type") #, sin_listing.clean$)
generate_price_boxplot(list_after_2019.sin, "Singapore", "Property_Type") #, sin_listing.clean$)
generate_price_boxplot(list_after_2019.sin, "Singapore", "Capacity") #, sin_listing.clean$)
generate_price_boxplot(list_after_2019.sin, "Singapore", "Beds") #, sin_listing.clean$)
generate_price_boxplot(list_after_2019.sin, "Singapore", "neighbourhood_cleansed") #, sin_listing.clean$)
generate_price_boxplot(list_after_2019.sin, "Singapore", "Reviews") #, sin_listing.clean$)
```

Seems like a single boat in Bukit Merah area (possibly next to the marina at Keppel Bay) has a very high price, at $2500/ night. Let's look that one up more closely.

```{r echo=TRUE, warning=FALSE, paged.print=TRUE}
# head(list_after_2019.sin %>% arrange(desc(Price)))
# head(list_after_2019.sin %>% arrange(desc(reviews_since_2019)))
head(list_after_2019.sin %>% filter(Property_Type == "Boat") %>% arrange (desc(reviews_since_2019)))
head(list_after_2019.sin %>% group_by(id, Property_Type, bookings_since_2019) %>% summarise(percent_of_total = bookings_since_2019*100/sum(list_after_2019.sin$bookings_since_2019)) %>% filter(Property_Type == "Boat") %>% arrange (desc(bookings_since_2019)))
```

There are four boats listed on Airbnb Singapore. Together, they form roughly 2% of all bookings since 2019.

## 2.3 Correlation Matrices
```{r warning=FALSE, paged.print=TRUE}
list_of_vars = c("earnings_since_2019","Rating", "Reviews", "Beds", "Capacity", "host_acceptance_rate", "host_Superhost","Amenities_Wifi","Amenities_Shampoo","Amenities_Kitchen","Amenities_Long_Term","Amenities_Washer","Amenities_HairDryer", "Amenities_HotWater", "Amenities_TV", "Amenities_AC", "hv_email",  "hv_reviews", "Shared_ind", "House_ind", "Private_ind")#, "reviews_since_2019","bookings_since_2019") #, "hood_factor")
# list_after_2019.sin %>% select_(.dots = c(list_of_vars), "Price")
```

```{r}
vars_list.sin = list_after_2019.sin %>% select_(.dots = c(list_of_vars,"Price")) %>% na.omit()
vars_list.tpe = list_after_2019.tpe %>% select_(.dots = c(list_of_vars,"Price")) %>% na.omit()
vars_list.hkg = list_after_2019.hkg %>% select_(.dots = c(list_of_vars,"Price")) %>% na.omit()
vars_list.nrt = list_after_2019.nrt %>% select_(.dots = c(list_of_vars,"Price")) %>% na.omit()

# vars_list.sin
```
```{r}
paint_corrleations <- function(listing)
{
  # chart.Correlation(listing, histogram=TRUE, pch=19)
  corrplot::corrplot(cor(listing, use = "complete.obs"), method="square", type="lower")
}
paint_corrleations(vars_list.sin)
paint_corrleations(vars_list.nrt)
paint_corrleations(vars_list.tpe)
paint_corrleations(vars_list.hkg)
```

# 3 Modelling

## 3.1 Principal Components Regression - Predicting Price
Principal Components Regression could find M linear combinations ("principal components") of our predictors (`list_of_vars`) and then use least squares to fit a linear regression model.

```{r}
set.seed(1)

pcr_model <- function (listings, city)
{
  pcr_model <- pcr( data=listings, scale=TRUE, validation="CV", Price ~ reviews_since_2019 + Rating + host_acceptance_rate +host_Superhost + reviews_since_2019 + Shared_ind + House_ind + Private_ind + Amenities_Wifi + hv_email) 
  
  summary(pcr_model)
  plot(pcr_model)
  validationplot(pcr_model, val.type="MSEP")
  validationplot(pcr_model, val.type="R2")
  print(paste("MAE for", city,":", mae(listings$Price, predict(pcr_model))))
  return (pcr_model)
}
pcr_model.sin <-pcr_model(list_after_2019.sin, "Singapore")
pcr_model.hkg <-pcr_model(list_after_2019.hkg, "Hong Kong")
pcr_model.tpe <-pcr_model(list_after_2019.tpe, "Taipei")
pcr_model.nrt <-pcr_model(list_after_2019.nrt, "Tokyo")
```

The mean absolute error for each city is about $108.

## 3.2 Predicting Earnings
```{r}
set.seed(1)

pcr_model_earnings <- function (listings, city)
{
  pcr_model <- pcr( data=listings, scale=TRUE, validation="CV", earnings_since_2019 ~ Price + reviews_since_2019 + Rating + host_acceptance_rate +host_Superhost + reviews_since_2019 + Shared_ind + House_ind + Private_ind + Amenities_Wifi + hv_email) #", 
                   # "host_Superhost", "no_of_am","Amenities_Wifi","Amenities_Shampoo","Amenities_Kitchen","Amenities_Long_Term","Amenities_Washer",
                   # "Amenities_HairDryer", "Amenities_HotWater", "Amenities_TV", "Amenities_AC", "hv_email",  "hv_facebook", "hv_reviews",
                   # "hv_manual_offline", "hv_manual_jumio", "hv_manual_off_gov", "hv_manual_gov", "hv_manual_work_email", "no_of_vf", "Days_since_last_review",
                   # , "reviews_since_2019","bookings_since_2019")
  
  summary(pcr_model)
  plot(pcr_model)
  validationplot(pcr_model, val.type="MSEP")
  validationplot(pcr_model, val.type="R2")
  print(paste("MAE for", city,":", mae(listings$earnings_since_2019, predict(pcr_model))))
  return (pcr_model)
}
pcr_model.sin <-pcr_model_earnings(list_after_2019.sin, "Singapore")
pcr_model.hkg <-pcr_model_earnings(list_after_2019.hkg, "Hong Kong")
pcr_model.tpe <-pcr_model_earnings(list_after_2019.tpe, "Taipei")
pcr_model.nrt <-pcr_model_earnings(list_after_2019.nrt, "Tokyo")
```

# This doesn't work
```{r}

# lm0 <- lm(Price ~ Capacity, data = sin_listing.clean)
# summary(lm0)
# stargazer(lm0, type = "text")
#
# ggplot(data = sin_listing.clean, aes(x = Capacity, y = Price)) + geom_point(aes(size=3)) +
# scale_colour_hue(l=50) + # Use a slightly darker palette than normal
# geom_smooth(method=lm,   # Add linear regression lines
#            se=TRUE,    #  add shaded confidence region
#            fullrange=TRUE) +
# theme(axis.text.x = element_text(size=15), axis.text.y = element_text(size=15),
#         axis.title=element_text(size=15,face="bold"))
# # vif(lm0)
# ```
# ```{r}
#
# # The moderating effect of type of room. Lets model that.
#
# lm1 <- lm(Price ~ Private_ind + House_ind, data = sin_listing.clean)
# summary(lm1)
# stargazer(lm1, type = "text")
# ```
# ```{r}
# #Regression with Capacity and Dummy Variables for type of room:
#
# lm2 <- lm(Price ~ Capacity + Private_ind + House_ind, data = sin_listing.clean)
# summary(lm2)
# stargazer(lm2, type = "text")
```

```{r}
#Regression with Capacity,Dummy Variables and interaction between the two:
# lm3 <- lm(Price ~ Capacity+Private_ind + House_ind+P_Cap+H_Cap, data = sin_listing.clean)
# summary(lm3)
# stargazer(lm3,type = "text")

```

# Define function to clean 
Comments from Kevin: I have created 2 way of cleaning

- clean_subset_including --> To select the variables we want (I used this for Lasso since built-in stepwise regression function automatically creates dummy variables)

- clean_subset_including --> To select the variables we don't want (I used this for Lasso since there is a lot of dummy variables and I rather exclude those not needed)
```{r}
### For checking number of missing data
# md.pattern(list_after_2019.country)

clean_subset_including <- function(list_after_2019.country) {
  
### Arbitrary selection of a list of variables 
  selecting_columns <- list_after_2019.country[,c("Price","Reviews","Beds","Capacity","Monthly_Reviews","Property_Type","Room_Type","Rating","neighbourhood_cleansed","host_response_time","host_acceptance_rate","host_Superhost","no_of_am","Amenities_Wifi","Amenities_Shampoo","Amenities_Kitchen","Amenities_Long_Term","Amenities_Washer","Amenities_HairDryer","Amenities_HotWater","Amenities_TV","Amenities_AC","hv_email","hv_phone","hv_facebook","hv_reviews","hv_manual_offline","hv_manual_jumio","hv_manual_off_gov","hv_manual_gov","hv_manual_work_email","no_of_vf","Days_since_last_review","Capacity_Sqr","Beds_Sqr","ln_Beds","ln_Capacity","ln_Rating","Shared_ind","House_ind","Private_ind","Capacity_x_Shared_ind","H_Cap","P_Cap","ln_Capacity_x_Shared_ind","ln_Capacity_x_House_ind","ln_Capacity_x_Private_ind","reviews_since_2019","bookings_since_2019", "earnings_since_2019" )]

### Removing rows with blanks instead of imputing
selecting_columns <- na.omit(selecting_columns)
selecting_columns$Property_Type <- as.factor(selecting_columns$Property_Type)
selecting_columns$neighbourhood_cleansed  <- as.factor(selecting_columns$neighbourhood_cleansed )
selecting_columns$host_response_time <- as.factor(selecting_columns$host_response_time)

  return(selecting_columns)
}

###############################################################################################################

clean_subset_excluding <- function(list_after_2019.country) {
  
selecting_columns <- list_after_2019.country[,!names(list_after_2019.country) %in%  c('id','ln_Price','host_response_time','host_response_rate','host_verifications','Baths','Baths_Sqr','ln_Baths','latitude','longitude','neighbourhood_cleansed','amenities','last_review','1','0','host_response_hours','host_response_day','host_response_few_days')]

selecting_columns <- na.omit(selecting_columns)
  return(selecting_columns)
}

###############################################################################################################

# For Stepwise Regression Input
list_after_2019.sin_step <- clean_subset_including(list_after_2019.sin)
list_after_2019.hkg_step <- clean_subset_including(list_after_2019.hkg)
list_after_2019.nrt_step <- clean_subset_including(list_after_2019.nrt)
list_after_2019.tpe_step <- clean_subset_including(list_after_2019.tpe)

# For Lasso Regression Input
list_after_2019.sin_clean <- clean_subset_excluding(list_after_2019.sin_remove)
list_after_2019.hkg_clean <- clean_subset_excluding(list_after_2019.hkg_remove)
list_after_2019.nrt_clean <- clean_subset_excluding(list_after_2019.nrt_remove)
list_after_2019.tpe_clean <- clean_subset_excluding(list_after_2019.tpe_remove)
```

# Feature Selection - Backward Stepwise Regression
Comments from Kevin: Can help to double check if the R-squared value looks too high at 0.80 for a real-world dataset?

```{r}
stepwise_regression_model <- function(list_after_2019.country_step) {
  
  #Define Smallest and Full Model 
  minmod = lm(Price~1, data = list_after_2019.country_step)
  fullmod = lm(Price~. , data = list_after_2019.country_step)
  
  # Using BIC: k=log(nobs(fullmod), Using AIC: k=2
  backward_regression_model <- step(fullmod, scope = list(lower = minmod, upper = fullmod),direction = "backward", k=log(nobs(fullmod)), trace=F)
  return (backward_regression_model)
}


summary(stepwise_regression_model(list_after_2019.sin_step))
summary(stepwise_regression_model(list_after_2019.hkg_step))
summary(stepwise_regression_model(list_after_2019.nrt_step))
summary(stepwise_regression_model(list_after_2019.tpe_step))


# Putting forward stepwise regression in comments in case we need to use 
# forward_regression = step(minmod, scope = list(lower = minmod, upper = fullmod),direction = "forward", k=log(nobs(fullmod)), trace=F)
# summary(forward_regression)

```
# Lasso Regreesion
```{r}
set.seed(100)
library(glmnet)
lasso_cv_sin= cv.glmnet(as.matrix(list_after_2019.sin_clean[,-1]),list_after_2019.sin_clean[,1],family="gaussian",alpha=1, nfolds=10)
lasso_cv_hkg= cv.glmnet(as.matrix(list_after_2019.hkg_clean[,-1]),list_after_2019.hkg_clean[,1],family="gaussian",alpha=1, nfolds=10)
lasso_cv_nrt= cv.glmnet(as.matrix(list_after_2019.nrt_clean[,-1]),list_after_2019.nrt_clean[,1],family="gaussian",alpha=1, nfolds=10)
lasso_cv_tpe= cv.glmnet(as.matrix(list_after_2019.tpe_clean[,-1]),list_after_2019.tpe_clean[,1],family="gaussian",alpha=1, nfolds=10)

coef(lasso_cv_sin,s=lasso_cv_sin$lambda.min)
coef(lasso_cv_hkg,s=lasso_cv_hkg$lambda.min)
coef(lasso_cv_nrt,s=lasso_cv_nrt$lambda.min)
coef(lasso_cv_tpe,s=lasso_cv_tpe$lambda.min)

```
# Training the Model

```{r}
set.seed(123)
# Splitting the data into 80:20
train_idx <- sample(1:nrow(list_after_2019.sin_clean), 0.8*nrow(list_after_2019.sin_clean)) 
training_sin <- list_after_2019.sin_clean[train_idx,]
testing_sin <- list_after_2019.sin_clean[-train_idx,]

final_model_sin <- lm(formula = Price ~ ln_Price + Property_Type + ln_Capacity_x_Private_ind + 
    Capacity + Amenities_Wifi + Amenities_Shampoo + Amenities_AC + 
    no_of_am + Amenities_Kitchen + hv_email, data = training_sin)
summary(final_model_sin)

```
# Checking Predictive Power and Mean absolute prediction error (MAE)
Comments from Kevin: Calculated the MAE value, but this number is probably more useful if its a comparison between models.

```{r}
predict_sin <- predict(final_model_sin,newdata=testing_sin)
MAE_sin <- mean(abs(predict_sin - testing_sin$Price))
```

# Checking for Assumptions on Outliers and Multicollinearity
Comments from Kevin: There seems to be out-lying points and issue of multicollinearity - we could either try to resolve or in the interest of time, just state that this is subsequently something we could look into 

```{r}
cook=cooks.distance(final_model_sin)
plot(cook,type='h',lwd=3,ylab="Cook's Distance")
vif(final_model_sin)
```